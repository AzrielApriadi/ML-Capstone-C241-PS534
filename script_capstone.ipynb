{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjYIu36QCYvMb90941Sr7x"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "DC_zRTiIfeAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSZPvKNkfwKC",
        "outputId": "34211513-8353-4849-cda1-5817b335826f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXo4SvQJMtme"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "def split_data(data_dir, train_dir, val_dir, split_ratio=0.8):\n",
        "    \"\"\"\n",
        "    Splits data into training and validation sets.\n",
        "\n",
        "    Parameters:\n",
        "    - data_dir: Directory containing the dataset, structured with subdirectories for each class.\n",
        "    - train_dir: Directory where the training data will be saved.\n",
        "    - val_dir: Directory where the validation data will be saved.\n",
        "    - split_ratio: Proportion of data to be used for training (default is 0.7 for a 70-30 split).\n",
        "    \"\"\"\n",
        "    if not os.path.exists(train_dir):\n",
        "        os.makedirs(train_dir)\n",
        "    if not os.path.exists(val_dir):\n",
        "        os.makedirs(val_dir)\n",
        "\n",
        "    for class_dir in os.listdir(data_dir):\n",
        "        class_path = os.path.join(data_dir, class_dir)\n",
        "\n",
        "        if os.path.isdir(class_path):\n",
        "            files = os.listdir(class_path)\n",
        "            random.shuffle(files)\n",
        "\n",
        "            split_point = int(len(files) * split_ratio)\n",
        "            train_files = files[:split_point]\n",
        "            val_files = files[split_point:]\n",
        "\n",
        "            train_class_dir = os.path.join(train_dir, class_dir)\n",
        "            val_class_dir = os.path.join(val_dir, class_dir)\n",
        "\n",
        "            if not os.path.exists(train_class_dir):\n",
        "                os.makedirs(train_class_dir)\n",
        "            if not os.path.exists(val_class_dir):\n",
        "                os.makedirs(val_class_dir)\n",
        "\n",
        "            for file in train_files:\n",
        "                shutil.copy(os.path.join(class_path, file), os.path.join(train_class_dir, file))\n",
        "\n",
        "            for file in val_files:\n",
        "                shutil.copy(os.path.join(class_path, file), os.path.join(val_class_dir, file))\n",
        "\n",
        "# Example usage\n",
        "data_dir = '/content/drive/MyDrive/Datasets'\n",
        "train_dir = '/content/drive/MyDrive/Capstone Dataset 80:20/training'\n",
        "val_dir = '/content/drive/MyDrive/Capstone Dataset 80:20/validation'\n",
        "split_data(data_dir, train_dir, val_dir, split_ratio=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import shutil\n",
        "# import random\n",
        "\n",
        "# def split_data(data_dir, train_dir, val_dir, split_ratio=0.8):\n",
        "#     \"\"\"\n",
        "#     Splits data into training and validation sets, avoiding duplicate data.\n",
        "\n",
        "#     Parameters:\n",
        "#     - data_dir: Directory containing the dataset, structured with subdirectories for each class.\n",
        "#     - train_dir: Directory where the training data will be saved.\n",
        "#     - val_dir: Directory where the validation data will be saved.\n",
        "#     - split_ratio: Proportion of data to be used for training (default is 0.8 for an 80-20 split).\n",
        "#     \"\"\"\n",
        "#     if not os.path.exists(train_dir):\n",
        "#         os.makedirs(train_dir)\n",
        "#     if not os.path.exists(val_dir):\n",
        "#         os.makedirs(val_dir)\n",
        "\n",
        "#     # Dictionary to store file paths assigned to each set\n",
        "#     train_files_assigned = {}\n",
        "#     val_files_assigned = {}\n",
        "\n",
        "#     for class_dir in os.listdir(data_dir):\n",
        "#         class_path = os.path.join(data_dir, class_dir)\n",
        "\n",
        "#         if os.path.isdir(class_path):\n",
        "#             files = os.listdir(class_path)\n",
        "#             random.shuffle(files)\n",
        "\n",
        "#             split_point = int(len(files) * split_ratio)\n",
        "#             train_files = files[:split_point]\n",
        "#             val_files = files[split_point:]\n",
        "\n",
        "#             train_class_dir = os.path.join(train_dir, class_dir)\n",
        "#             val_class_dir = os.path.join(val_dir, class_dir)\n",
        "\n",
        "#             if not os.path.exists(train_class_dir):\n",
        "#                 os.makedirs(train_class_dir)\n",
        "#             if not os.path.exists(val_class_dir):\n",
        "#                 os.makedirs(val_class_dir)\n",
        "\n",
        "#             for file in train_files:\n",
        "#                 if file not in train_files_assigned:\n",
        "#                     train_files_assigned[file] = True\n",
        "#                     shutil.copy(os.path.join(class_path, file), os.path.join(train_class_dir, file))\n",
        "\n",
        "#             for file in val_files:\n",
        "#                 if file not in val_files_assigned:\n",
        "#                     val_files_assigned[file] = True\n",
        "#                     shutil.copy(os.path.join(class_path, file), os.path.join(val_class_dir, file))\n",
        "\n",
        "# # Example usage\n",
        "# data_dir = '/content/drive/MyDrive/Datasets'\n",
        "# train_dir = '/content/drive/MyDrive/Capstone Dataset 80:20/training'\n",
        "# val_dir = '/content/drive/MyDrive/Capstone Dataset 80:20/validation'\n",
        "# split_data(data_dir, train_dir, val_dir, split_ratio=0.8)"
      ],
      "metadata": {
        "id": "h14l9CFAj21C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shared_drive_path = '/content/drive/MyDrive/Capstone Dataset 80:20/training'\n",
        "if os.path.exists(shared_drive_path):\n",
        "    # List all items in the shared_drive_path\n",
        "    all_items = os.listdir(shared_drive_path)\n",
        "\n",
        "    # Filter out directories only\n",
        "    directories = [item for item in all_items if os.path.isdir(os.path.join(shared_drive_path, item))]\n",
        "\n",
        "    # Iterate through each directory and count the number of items\n",
        "    for directory in directories:\n",
        "        dir_path = os.path.join(shared_drive_path, directory)\n",
        "        num_items = len(os.listdir(dir_path))\n",
        "        print(f\"Training Directory: {directory}, Number of items: {num_items}\")\n",
        "else:\n",
        "    print(f\"The directory {shared_drive_path} does not exist.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXQj1TAhhKyc",
        "outputId": "7f1c3849-1b32-4695-b959-54531f2ec148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Directory: Cabai Merah besar, Number of items: 164\n",
            "Training Directory: Cabai Hijau Besar, Number of items: 176\n",
            "Training Directory: Cabai Keriting, Number of items: 159\n",
            "Training Directory: Cabai Rawit, Number of items: 167\n",
            "Training Directory: Cabai gendot (habanero), Number of items: 161\n",
            "Training Directory: Cabai Jalapeno, Number of items: 161\n",
            "Training Directory: Cabai Paprika, Number of items: 169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shared_drive_path = '/content/drive/MyDrive/Capstone Dataset 80:20/validation'\n",
        "if os.path.exists(shared_drive_path):\n",
        "    # List all items in the shared_drive_path\n",
        "    all_items = os.listdir(shared_drive_path)\n",
        "\n",
        "    # Filter out directories only\n",
        "    directories = [item for item in all_items if os.path.isdir(os.path.join(shared_drive_path, item))]\n",
        "\n",
        "    # Iterate through each directory and count the number of items\n",
        "    for directory in directories:\n",
        "        dir_path = os.path.join(shared_drive_path, directory)\n",
        "        num_items = len(os.listdir(dir_path))\n",
        "        print(f\"Validation Directory: {directory}, Number of items: {num_items}\")\n",
        "else:\n",
        "    print(f\"The directory {shared_drive_path} does not exist.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQHuIO_nhUKK",
        "outputId": "8684f408-3907-4ca6-f48a-7648df306826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Directory: Cabai Merah besar, Number of items: 42\n",
            "Validation Directory: Cabai Hijau Besar, Number of items: 44\n",
            "Validation Directory: Cabai Keriting, Number of items: 40\n",
            "Validation Directory: Cabai Rawit, Number of items: 42\n",
            "Validation Directory: Cabai gendot (habanero), Number of items: 41\n",
            "Validation Directory: Cabai Jalapeno, Number of items: 41\n",
            "Validation Directory: Cabai Paprika, Number of items: 43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import hashlib\n",
        "\n",
        "def find_duplicates(directory):\n",
        "    \"\"\"\n",
        "    Find duplicate files within a directory.\n",
        "\n",
        "    Parameters:\n",
        "    - directory: Directory containing the dataset.\n",
        "\n",
        "    Returns:\n",
        "    - A list of duplicate file paths.\n",
        "    \"\"\"\n",
        "    duplicates = []\n",
        "    hash_dict = {}\n",
        "\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for filename in files:\n",
        "            file_path = os.path.join(root, filename)\n",
        "\n",
        "            # Calculate the hash value of the file\n",
        "            with open(file_path, 'rb') as f:\n",
        "                file_hash = hashlib.sha256(f.read()).hexdigest()\n",
        "\n",
        "            # Check if the hash value already exists in the dictionary\n",
        "            if file_hash in hash_dict:\n",
        "                duplicates.append(file_path)\n",
        "            else:\n",
        "                hash_dict[file_hash] = file_path\n",
        "\n",
        "    return duplicates\n",
        "\n",
        "# Example usage\n",
        "train_dir = '/content/drive/MyDrive/Capstone Dataset 80:20/training'\n",
        "val_dir = '/content/drive/MyDrive/Capstone Dataset 80:20/validation'\n",
        "\n",
        "# Find duplicates in both training and validation sets\n",
        "print(\"Duplicates between training and validation sets:\")\n",
        "train_files = set(find_duplicates(train_dir))\n",
        "val_files = set(find_duplicates(val_dir))\n",
        "\n",
        "common_duplicates = train_files.intersection(val_files)\n",
        "if common_duplicates:\n",
        "    for duplicate in common_duplicates:\n",
        "        print(duplicate)\n",
        "else:\n",
        "    print(\"No duplicates found between training and validation sets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD7Rpph6iWQY",
        "outputId": "d9359d2c-38cd-409a-d290-3024b82def1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicates between training and validation sets:\n",
            "No duplicates found between training and validation sets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "import os\n",
        "\n",
        "def calculate_file_hash(file_path):\n",
        "    \"\"\"\n",
        "    Calculate the SHA-256 hash of a file.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path: Path to the file.\n",
        "\n",
        "    Returns:\n",
        "    - The SHA-256 hash of the file.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return hashlib.sha256(f.read()).hexdigest()\n",
        "\n",
        "def find_duplicates(train_dir, val_dir):\n",
        "    \"\"\"\n",
        "    Find duplicate files between training and validation sets.\n",
        "\n",
        "    Parameters:\n",
        "    - train_dir: Directory containing the training data.\n",
        "    - val_dir: Directory containing the validation data.\n",
        "\n",
        "    Returns:\n",
        "    - A list of duplicate file paths.\n",
        "    \"\"\"\n",
        "    train_hashes = {}\n",
        "    val_hashes = {}\n",
        "\n",
        "    # Calculate hashes for training set\n",
        "    for root, _, files in os.walk(train_dir):\n",
        "        for filename in files:\n",
        "            file_path = os.path.join(root, filename)\n",
        "            file_hash = calculate_file_hash(file_path)\n",
        "            train_hashes[file_hash] = file_path\n",
        "\n",
        "    # Calculate hashes for validation set\n",
        "    for root, _, files in os.walk(val_dir):\n",
        "        for filename in files:\n",
        "            file_path = os.path.join(root, filename)\n",
        "            file_hash = calculate_file_hash(file_path)\n",
        "            val_hashes[file_hash] = file_path\n",
        "\n",
        "    # Find duplicates\n",
        "    duplicates = []\n",
        "    for file_hash, file_path in train_hashes.items():\n",
        "        if file_hash in val_hashes:\n",
        "            duplicates.append((file_path, val_hashes[file_hash]))\n",
        "\n",
        "    return duplicates\n",
        "\n",
        "# Example usage\n",
        "train_dir = '/content/drive/MyDrive/Capstone Dataset 80:20/training'\n",
        "val_dir = '/content/drive/MyDrive/Capstone Dataset 80:20/validation'\n",
        "\n",
        "duplicates = find_duplicates(train_dir, val_dir)\n",
        "if duplicates:\n",
        "    print(\"Duplicates found between training and validation sets:\")\n",
        "    for train_file, val_file in duplicates:\n",
        "        print(f\"Training file: {train_file}, Validation file: {val_file}\")\n",
        "else:\n",
        "    print(\"No duplicates found between training and validation sets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hmVXi-Qn2M9",
        "outputId": "a0ab87cc-fd09-43ee-e8f0-ac46168146f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicates found between training and validation sets:\n",
            "Training file: /content/drive/MyDrive/Capstone Dataset 80:20/training/Cabai Keriting/cabe-keriting-hijau-1-SESA_1-removebg-preview (1).jpg, Validation file: /content/drive/MyDrive/Capstone Dataset 80:20/validation/Cabai Keriting/cabe-keriting-hijau-1-SESA_1-removebg-preview.jpg\n"
          ]
        }
      ]
    }
  ]
}